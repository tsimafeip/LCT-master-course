{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13935de6-9e28-4f76-83da-6f72218e3bef",
      "metadata": {
        "id": "13935de6-9e28-4f76-83da-6f72218e3bef"
      },
      "source": [
        "# NNIA Assignment 8\n",
        "\n",
        "**DEADLINE: 19. 1. 2022 08:00 CET**\n",
        "Submission more than 10 minutes past the deadline will **not** be graded!\n",
        "\n",
        "- Trevor Atkins & trat00001@uni-saarland.de \n",
        "- Tsiamfei Prakapenka & tspr00001@uni-saarland.de \n",
        "- Hours of work per person: Prakapenka ~2.5h Atkins ~2h\n",
        "\n",
        "# Submission Instructions\n",
        "\n",
        "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
        "\n",
        "* Assignments are to be submitted in a **team of 2**.\n",
        "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
        "* Make sure you appropriately comment your code wherever required.\n",
        "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
        "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
        "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
        "* Only **one member** of the group should make the submisssion.\n",
        "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bfbdf7d-6c39-4eba-8b18-8cf8b8d86d8b",
      "metadata": {
        "id": "0bfbdf7d-6c39-4eba-8b18-8cf8b8d86d8b"
      },
      "source": [
        "## Neural Network Implementation: Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1a4fca1-2b85-4369-8fc3-4fd96a75b136",
      "metadata": {
        "id": "f1a4fca1-2b85-4369-8fc3-4fd96a75b136"
      },
      "source": [
        "### Theory Review (7 points)\n",
        "\n",
        "#### True or False\n",
        "\n",
        "After reviewing the lecture slides and Chapter 9 of the Deep learning book on [Convolutional Neural Networks](http://www.deeplearningbook.org/contents/convnets.html), determine whether each of the following statements is **True (T)** or **False (F)**. **Also provide a brief justification as to why you think so.**\n",
        "\n",
        "a) Pooling needs to be removed for handling inputs of varying size. *(1 pt)* \n",
        "\n",
        "b) Given a multilayered convolution neural network, a cell in a second convolutional layer has the same-sized receptive field as a cell in the first convolutional layer. *(1 pt)*\n",
        "\n",
        "c) In the context of edge detection, a convolutional neural network learns features for each pixel separately. *(1 pt)*\n",
        "\n",
        "d) There is an exponential increase in kernel parameters when convolutional neural networks's capabilities are increased to handle transformations like rotation, scaling etc. *(1 pt)*\n",
        "\n",
        "#### Convolutional Neural Networks Parameters\n",
        "\n",
        "a) In your own words, how are the concepts of *sparse interactions*, *parameter sharing*, and *equivariant representations* applied to convolution neural networks? *(1 pts)*\n",
        "\n",
        "\n",
        "b) When applying CNN on language sequences, why is it useful to combine kernels of different size? *(1 pts)*\n",
        "\n",
        "\n",
        "c) If a pixel represent an input unit for an image input to a CNN, what can be considered analogous to a pixel for a linguistic data input? *(1 pts)*  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0c8cc3-395d-4bf2-b9c4-0f61129c714d",
      "metadata": {
        "id": "7a0c8cc3-395d-4bf2-b9c4-0f61129c714d"
      },
      "source": [
        "## <font color=\"green\">Answers</font> \n",
        "Theory Review <br>\n",
        "a.) False - this is because pooling can be essential for handling various input sizes depending on the task such as classifying images of variable size - the input to the classiﬁcation layer must have a ﬁxed size. By varying the size of an offset between pooling regions, the classiﬁcation layer always receives the same number of summary statistics regardless of the input size. \n",
        "\n",
        "b.) True - the receptive field is spatially contiguous assigned to a hidden unit or cell.\n",
        "\n",
        "c.) True - each pixel in the original image is subtracted by a value of a neighboring pixel.\n",
        "\n",
        "d.) False - we can initially learn multiple filters and then due to pooling just different areas of image will affect final result.\n",
        "\n",
        "Convolutional Neural Networks Parameters\n",
        "\n",
        "a.) Sparse interaction in terms of CNN means that input does not influence all the output tensors of the next layer as in FFNN. Parameter sharing means that we apply the same filter in different places instead of learning multiple set of parameters for applying in exactly one place. Equivariant representation means that after changing the input the output will be changed respectively (brighter, for instance) without a need to retrain the model.\n",
        "\n",
        "b.) We may want to extract information on different levels. Presumably, small kernels are useful for char/morpheme level analysis, while bigger are good for word/sentence level analysis.\n",
        "\n",
        "c.) A morpheme could be considered analogous to a pixel for a linguistic text data input such as a corpus of sentences, or a phoneme (or a unit of wavelength/frequency of audio input) for linguistic audio data input such as a database of spoken words or phrases. Also, a single char can be considered as a pixel, in my opinion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56cadd1e-83ee-4cbd-88ee-0d28cf02e56e",
      "metadata": {
        "id": "56cadd1e-83ee-4cbd-88ee-0d28cf02e56e"
      },
      "source": [
        "### Implementation (3 pts)\n",
        "\n",
        "In this exercise, we will continue to work with [the PyTorch Datasets Class](https://pytorch.org/vision/stable/datasets.html) to obtain\n",
        "[the CIFAR10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Instead of the simple neural network from the previous assignment, we are going to implement a convolution neural networks (CNN) model to classify the images in this dataset into their proper classes.\n",
        "\n",
        "Your CNN model will have the following architecture:\n",
        "\n",
        "\n",
        "* It has five convolution blocks. \n",
        "* Each block consists of *convolution*, *max pooling* and *ReLU* operation in that order. \n",
        "* We will use 3×3 kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they maintain the spatial dimensions. \n",
        "* Max pooling operations are done with 2×2 kernels, with a stride of 2, thereby halving the spatial resolution each time. \n",
        "* Finally, five stacking these five blocks leads to a 512 × 1 × 1 feature map. \n",
        "* Classification is achieved by a fully connected layer. \n",
        "\n",
        "Implement a class *ConvNet* to define the model described. The ConvNet takes 32 × 32 color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces 10-class classification. We will train the convolutional neural networks on the CIFAR-10 dataset. Feel free to incorporate drop-put, batch normalization, and early stopping if desired. Evaluate your trained model on the test set and report your findings. Hyperparameter values and some helper functions are provided in the `solution.py` file where you can continue your implementation.\n",
        "\n",
        "For loss, you can use nn.CrossEntropyLoss() and for optimization, you can use the Adam optimizer with the learning rate of 2e-3 and weight decay of 0.001. \n",
        "       \n",
        "**Note**: To speed up trainining on the entire dataset, you may want an access to a GPU (CPU runtime > 10 hrs vs < 5 mins GPU). We recommend you make use of [Google Colab](https://colab.research.google.com/?utm_source=scs-index). You may also partition the dataset and work with a smaller sample size if you do not have access to a GPU. For tutorials on how to use Google Colab for deep learning and their file organization, you may find following tutorials helpful: \n",
        "\n",
        "* [Neptune AI Blog: Using Google Colab for Deep Learning](https://neptune.ai/blog/how-to-use-google-colab-for-deep-learning-complete-tutorial)  \n",
        "* [Neptune AI Blog: Dealing with Files in Google Colab](https://neptune.ai/blog/google-colab-dealing-with-files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "feef094d-d753-4e3d-b5ef-d720810dae31",
      "metadata": {
        "id": "feef094d-d753-4e3d-b5ef-d720810dae31"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ed646d5",
      "metadata": {
        "id": "6ed646d5"
      },
      "outputs": [],
      "source": [
        "# from solution import your functions\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5c61dfa",
      "metadata": {
        "id": "b5c61dfa"
      },
      "outputs": [],
      "source": [
        "# global constants\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "INPUT_SIZE = 3\n",
        "NUM_CLASSES = 10\n",
        "HIDDEN_SIZE = [128, 512, 512, 512, 512, 512]\n",
        "NUM_EPOCHS = 20  # default is 20, changeable via cl\n",
        "BATCH_SIZE = 32\n",
        "LR = 2e-3\n",
        "LR_DECAY = 0.95\n",
        "REG = 0.001\n",
        "TRAINING_SIZE = 49000\n",
        "VAL_SIZE = 1000\n",
        "DROP_OUT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c735a188",
      "metadata": {
        "id": "c735a188"
      },
      "outputs": [],
      "source": [
        "def get_cifar10_dataset(val_size: int = VAL_SIZE, batch_size: int = BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Load and transform the CIFAR10 dataset. Make Validation set. Create dataloaders for\n",
        "    train, test, validation sets. Only train_loader uses batch_size of 200, val_loader and\n",
        "    test_loader have 1 batch (i.e. batch_size == len(val_set) etc.)\n",
        "\n",
        "    DO NOT CHANGE THE CODE IN THIS FUNCTION. YOU MAY CHANGE THE BATCH_SIZE PARAM IF NEEDED.\n",
        "\n",
        "    If you get an error related num_workers, you may change that parameter to a different value.\n",
        "\n",
        "    :param val_size: size of the validation partition\n",
        "    :param batch_size: number of samples in a batch\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # the datasets.CIFAR getitem actually returns img in PIL format\n",
        "    # no need to get to Tensor since we're working with our own model and not PyTorch\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703233, 0.24348505, 0.26158768))\n",
        "                                    ])\n",
        "\n",
        "    # Load the train_set and test_set from PyTorch, transform each sample to a flattened array\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "    classes = train_set.classes\n",
        "\n",
        "    # Split data and define train_loader, test_loader, val_loader\n",
        "    train_size = len(train_set) - val_size\n",
        "    train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                               shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=len(test_set),\n",
        "                                              shuffle=False, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=val_size,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader, val_loader, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fe285fc7",
      "metadata": {
        "id": "fe285fc7"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes CNN.\"\"\"\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(INPUT_SIZE, HIDDEN_SIZE[0], (3, 3), padding=1)\n",
        "        self.conv2 = nn.Conv2d(HIDDEN_SIZE[0], HIDDEN_SIZE[1], (3, 3), padding=1)\n",
        "        self.conv3 = nn.Conv2d(HIDDEN_SIZE[1], HIDDEN_SIZE[2], (3, 3), padding=1)\n",
        "        self.conv4 = nn.Conv2d(HIDDEN_SIZE[2], HIDDEN_SIZE[3], (3, 3), padding=1)\n",
        "        self.conv5 = nn.Conv2d(HIDDEN_SIZE[3], HIDDEN_SIZE[4], (3, 3), padding=1)\n",
        "\n",
        "        self.fc = nn.Linear(HIDDEN_SIZE[4], NUM_CLASSES)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward path.\"\"\"\n",
        "        # [BATCH_SIZE, 3, 32, 32]\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # [BATCH_SIZE, 128, 16, 16]\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # [BATCH_SIZE, 512, 8, 8]\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        # [BATCH_SIZE, 512, 4, 4]\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        # [BATCH_SIZE, 512, 2, 2]\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        # [BATCH_SIZE, 512, 1, 1]\n",
        "        x = torch.flatten(x, 1)\n",
        "        # [BATCH_SIZE, 512]\n",
        "        x = self.fc(x)\n",
        "\n",
        "        # [BATCH_SIZE, 10]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "112134ed",
      "metadata": {
        "id": "112134ed"
      },
      "outputs": [],
      "source": [
        "def training(cnn_model: ConvNet, trainloader: torch.utils.data.DataLoader, \n",
        "             criterion, optimizer: optim.Optimizer):\n",
        "    \"\"\"Trains model and reports metrics.\"\"\"\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        total_batches = running_loss = 0\n",
        "        for i, data in tqdm(enumerate(trainloader, 0)):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            total_batches += 1\n",
        "        print()\n",
        "        print(f'{epoch + 1} epoch loss: {running_loss / total_batches:.3f}')\n",
        "    print('Finished Training!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "28583b1d-6975-44db-b83d-347148329e7c",
      "metadata": {
        "id": "28583b1d-6975-44db-b83d-347148329e7c"
      },
      "outputs": [],
      "source": [
        "def evaluation(cnn_model: ConvNet, classes: List[str], \n",
        "               testloader: torch.utils.data.DataLoader):\n",
        "    \"\"\"Evaluation loop.\"\"\"\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            outputs = cnn_model(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader, val_loader, classes = get_cifar10_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "621d98b9683d4ae5a5476a7f14e34acf",
            "82efd4c07fc545de8c8eef09f0fea7e4",
            "e95a94241b604dec804dae68accc6cfe",
            "0ed7cda82fdf461da3560d3b36d4867d",
            "51ce023b3501460f8ead2a9010a997ab",
            "4cd0b8838f1d4080be235098d7635254",
            "f12e0faadff44124b0c33b138f75ef68",
            "ae5a8bf206f347b7b6f392e078e5458c",
            "1b1a43a22d3e4ba0a47581acd3de1880",
            "da539d60ebd545cc94dc6063967e94b3",
            "ea5808411a3c4ac9b94d219238966ee3"
          ]
        },
        "id": "Z01J17vxCBsl",
        "outputId": "4200a188-c0f7-4616-93fd-f3a079da98b6"
      },
      "id": "Z01J17vxCBsl",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "621d98b9683d4ae5a5476a7f14e34acf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = ConvNet().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=LR, weight_decay=0.001)"
      ],
      "metadata": {
        "id": "8Z1ohAhlCCqz"
      },
      "id": "8Z1ohAhlCCqz",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "YdW_lIgetyd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdW_lIgetyd5",
        "outputId": "9eb328ce-2e0e-4755-e8b2-bf825591ce3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1532it [00:34, 44.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1 epoch loss: 1.567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:33, 45.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2 epoch loss: 1.162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:33, 45.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3 epoch loss: 1.012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:34, 44.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4 epoch loss: 0.924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:34, 44.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epoch loss: 0.857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:34, 44.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6 epoch loss: 0.820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:34, 43.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7 epoch loss: 0.788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:34, 43.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8 epoch loss: 0.759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9 epoch loss: 0.741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:34, 43.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epoch loss: 0.726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "11 epoch loss: 0.715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "12 epoch loss: 0.708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "13 epoch loss: 0.695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "14 epoch loss: 0.687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "15 epoch loss: 0.672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "16 epoch loss: 0.671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "17 epoch loss: 0.668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "18 epoch loss: 0.661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "19 epoch loss: 0.651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "1532it [00:35, 43.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "20 epoch loss: 0.649\n",
            "Finished Training!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training(cnn_model, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(cnn_model, classes, val_loader)"
      ],
      "metadata": {
        "id": "tHwyGJ6sDwut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e61099a-91b1-4af7-c9c6-a2470d70343b"
      },
      "id": "tHwyGJ6sDwut",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class: airplane is 70.5 %\n",
            "Accuracy for class: automobile is 88.6 %\n",
            "Accuracy for class: bird  is 48.0 %\n",
            "Accuracy for class: cat   is 59.4 %\n",
            "Accuracy for class: deer  is 73.5 %\n",
            "Accuracy for class: dog   is 57.3 %\n",
            "Accuracy for class: frog  is 75.3 %\n",
            "Accuracy for class: horse is 88.5 %\n",
            "Accuracy for class: ship  is 79.5 %\n",
            "Accuracy for class: truck is 85.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iM2VTjJQ1tfZ",
      "metadata": {
        "id": "iM2VTjJQ1tfZ"
      },
      "source": [
        "### Bonus\n",
        "\n",
        "Implement a function to calculate the number of traininable parameters (1 pt) and another function to visualize the filter weights (1 pt). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "81c3358b",
      "metadata": {
        "id": "81c3358b"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"Returns number of trainable params for the model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3ecb85b7-51d3-4d57-a517-4105d18464fe",
      "metadata": {
        "id": "3ecb85b7-51d3-4d57-a517-4105d18464fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f873d41-4a04-4e8e-f9f4-7f9c30eef70b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7678474"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "count_parameters(cnn_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "assignment_8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "621d98b9683d4ae5a5476a7f14e34acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82efd4c07fc545de8c8eef09f0fea7e4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e95a94241b604dec804dae68accc6cfe",
              "IPY_MODEL_0ed7cda82fdf461da3560d3b36d4867d",
              "IPY_MODEL_51ce023b3501460f8ead2a9010a997ab"
            ]
          }
        },
        "82efd4c07fc545de8c8eef09f0fea7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e95a94241b604dec804dae68accc6cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cd0b8838f1d4080be235098d7635254",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f12e0faadff44124b0c33b138f75ef68"
          }
        },
        "0ed7cda82fdf461da3560d3b36d4867d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ae5a8bf206f347b7b6f392e078e5458c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b1a43a22d3e4ba0a47581acd3de1880"
          }
        },
        "51ce023b3501460f8ead2a9010a997ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da539d60ebd545cc94dc6063967e94b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 53048177.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea5808411a3c4ac9b94d219238966ee3"
          }
        },
        "4cd0b8838f1d4080be235098d7635254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f12e0faadff44124b0c33b138f75ef68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae5a8bf206f347b7b6f392e078e5458c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b1a43a22d3e4ba0a47581acd3de1880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da539d60ebd545cc94dc6063967e94b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea5808411a3c4ac9b94d219238966ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}