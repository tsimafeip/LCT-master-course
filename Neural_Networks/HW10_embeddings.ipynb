{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13935de6-9e28-4f76-83da-6f72218e3bef",
      "metadata": {
        "id": "13935de6-9e28-4f76-83da-6f72218e3bef"
      },
      "source": [
        "# NNIA Assignment 10\n",
        "\n",
        "**DEADLINE: 2. 2. 2022 08:00 CET**\n",
        "Submission more than 10 minutes past the deadline will **not** be graded!\n",
        "\n",
        "- Trevor Atkins & trat00001@uni-saarland.de \n",
        "- Tsimafei Prakapenka & tspr00001@uni-saarland.de \n",
        "- Hours of work per person: Atkins ~45min Prakapenka ~30min\n",
        "\n",
        "\n",
        "# Submission Instructions\n",
        "\n",
        "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
        "\n",
        "* Assignments are to be submitted in a **team of 2**.\n",
        "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
        "* Make sure you appropriately comment your code wherever required.\n",
        "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
        "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
        "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
        "* Only **one member** of the group should make the submisssion.\n",
        "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feef094d-d753-4e3d-b5ef-d720810dae31",
      "metadata": {
        "id": "feef094d-d753-4e3d-b5ef-d720810dae31"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c30bb375-058c-4253-aa17-7adfa4ea2ab6",
      "metadata": {
        "id": "c30bb375-058c-4253-aa17-7adfa4ea2ab6"
      },
      "source": [
        "# Putting it all together!\n",
        "\n",
        "Congratulations! You have made it the last assignment of the course! In this assignment we will review some theoritcal concepts in this course and also get more familiarized with word embeddings. Then we will put everything together in practice.\n",
        "\n",
        "You will need to review the contents in from the lecture slides and read [an article by Adrien Sieg about about static and contextualized embeddings](https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598) to answer the questions below. For each question, answers should be approximately between 2-4 sentences.\n",
        "\n",
        "Some other readings include:\n",
        "\n",
        "* [Neural Network Embeddings Explained](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)\n",
        "* [Machine learning core course, chapter on embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)\n",
        "\n",
        "The articles go quite deep into detail about the internal structure of BERT, but it is not necessary for this exercise. However, try to understand what is the function of each block.\n",
        "\n",
        "## Theory Review (2 points)\n",
        "\n",
        "Answer the questions with your own words, provide your own examples where possible.\n",
        "Points will be subtracted if you just copy the text.\n",
        "\n",
        "1. What is the benefit of an LSTM over RNN? Give a short explanation. (0.25 pt)\n",
        "\n",
        "The benefit of an LSTM over RNN is that it does not have the issue of exploding and vansishing of gradients that occur during backward propagation. This issue can arise from exponential stacking of weight matrices in the back propagation through time of an RNN with n inputs. THe LSTM has gates that impose the relevancy of information stored in a cell and hidden state to ensure the lack of vanishing or exploding gradients. \n",
        "\n",
        "2. Give short explanations of static and contextualized word embeddings. Provide examples. (0.5 pt)\n",
        "\n",
        "Static embeddings: vector for word remains the same in different contexts, it just encodes all of sences into one vector.\n",
        "\n",
        "Contextualized word embeddings: vector for word changes in different contexts. \n",
        "\n",
        "plant (like flower or tree) and plant (factory) will have the same static embedding, but different contextualized one.\n",
        "\n",
        "3. What are the advantages of contextualized (dynamic) word embeddings over static ones? (0.5 pt)\n",
        "\n",
        "The advantages of contextualized (dynamic) word embeddings over static ones include: \n",
        "\n",
        "- capturing word semantics in different contexts for the issue of polysemy\n",
        "- taking word order into account\n",
        "- possibility to handle subword/char units to solve out-of-vocabulary problem, neural nature of the contextualised embeddings allows it.\n",
        "\n",
        "4. What is transfer learning? On which task is a model (e.g. BERT) pre-trained? (Hint:\n",
        "read the article till the end) Can we pre-train without any task? Why / why not? (0.25 pt)\n",
        "\n",
        "Transfer learning is a technique where instead of training a model from scratch, models are pre-trained on a large dataset and fine-tuned for a specific task. \n",
        "\n",
        "A model (e.g. BERT) is pretrained on Masked Language Modeling with words replaced with [MASK] tokens, random words, or unchanged. The task is to  predict masked words or pairs of sentences for the task of predicting the relationship between consecutive sentences. Predicting relationship between senteces can be useful, for example, in Information Retrieval for query-document pairs relevance ranking.\n",
        "\n",
        "Model can be pretrained without any task - [mt5](!https://huggingface.co/docs/transformers/model_doc/mt5)), for example. In this case we just train internal language model which is necessarily followed by fast fine-tuning for downstream tasks like language identification or multilingual QA.\n",
        "\n",
        "\n",
        "5. Explain the attention mechanism. (0.5 pt)\n",
        "\n",
        "Attention mechanism allows decoder in seq2seq models to have 'weighted' access to all encoder states on every step of generating the next output token. This approach helped to solve the problem of using encoder output as a single vector during decoding. Also, there is a self-attention mechanism used in Transformer architectures to encode each token having an information about the other tokens in the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b15004-7b49-4394-b392-24c1c7758a87",
      "metadata": {
        "id": "37b15004-7b49-4394-b392-24c1c7758a87"
      },
      "source": [
        "## Hands-On Practice (8 points)\n",
        "\n",
        "Now that you have some background knowledge about word embeddings, you will build a model to perform a classification task on the [Amazon Product Review Dataset](https://huggingface.co/datasets/amazon_reviews_multi). The whole dataset consists of 200,000/5,000/5,000 samples per language and the labels (star ratings) are equally distributed sequentially in the dataset. Due to its large size, we **recommend** that you subset the dataset to work with to minimize loading and pre-processing time. Please take care to preserve the evenly distributed labels when you do this. \n",
        "\n",
        "While the dataset consists of reviews in multiple languages, we will work with the German subset of the dataset (configuration: 'de'). For resources on how to work with Huggingface Datasets and specifically how to specify slices/splits of the dataset, please refer to [Huggingface Documentation on Dataset Loading](https://huggingface.co/docs/datasets/loading_datasets.html) and [Tutorial on Splitting and Slicing](https://huggingface.co/docs/datasets/splits.html).\n",
        "\n",
        "To earn full credits for this part of the exercise, your task is to design and implement a model according to **1 or 2 of the following 3 schemes (either do BOTH 1 & 2 OR only \\# 3):** \n",
        "\n",
        "1. a classifer on top of Bag-of-Word and TF-IDF vectors ([BoW and TF-IDF](https://www.analyticsvidhya.com/blog/2021/07/bag-of-words-vs-tfidf-vectorization-a-hands-on-tutorial/))\n",
        "2. a classifier classifier on top of averaged static word embeddings (e.g. 300-dimensional vectors from [fastText for German](https://fasttext.cc/docs/en/crawl-vectors.html))\n",
        "3. a classifier on top of contextualized word embeddings (CLS token from [German BERT model for sentiment analysis](https://huggingface.co/oliverguhr/german-sentiment-bert), just run BERT once and store the CLS token representation; [Chris McCormick's tutorial on BERT for Text Classification](http://mccormickml.com/2019/07/22/BERT-fine-tuning/) may also be a useful resource for this.)\n",
        "\n",
        "\n",
        "Report and discuss your results among the 2 model choices. Make sure to **discuss** your rationales for your decisions regarding the following concepts:\n",
        "\n",
        "* model architecture\n",
        "* evaluation metrics (provide a summary table with performance of various architectures and hyperparameters), **important**\n",
        "* optimization techniques, hyper-parameters search\n",
        "\n",
        "**Important:** In all cases, provide a formatted table with the development and train performance. Although you are not penalized on low performance, your model should demonstrate that it is 'learning' during training (e.g. decreasing losses, increasing evaluation scores etc.) to earn full credits.\n",
        "\n",
        "## Bonus (2 points):\n",
        "Complete all 3 options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f4e3a48",
      "metadata": {
        "id": "2f4e3a48"
      },
      "outputs": [],
      "source": [
        "#from solution_tutor import \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Atkins_trat00001_Prakapenka_tspr00001_assignment_10 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}