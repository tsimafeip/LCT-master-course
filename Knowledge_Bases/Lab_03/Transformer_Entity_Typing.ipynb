{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c96517a2",
      "metadata": {
        "id": "c96517a2"
      },
      "source": [
        "## 0- Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOmu-H69MLy2",
        "outputId": "92a59eeb-d6c1-482f-c697-3accaf9c540e"
      },
      "id": "fOmu-H69MLy2",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f8a61d19",
      "metadata": {
        "id": "f8a61d19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "import wget\n",
        "import nltk\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-3fo5-pQW1K",
        "outputId": "9e0e9148-9a66-4b23-f986-414404095b0b"
      },
      "id": "3-3fo5-pQW1K",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8830e42b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8830e42b",
        "outputId": "92d92571-e646-4487-effc-b6c759b97d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "# check available device\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Running on\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9bcfa0b8",
      "metadata": {
        "id": "9bcfa0b8"
      },
      "outputs": [],
      "source": [
        "# define constants\n",
        "MENTION_OPEN_TOKEN = '<men>'\n",
        "MENTION_CLOSE_TOKEN = '</men>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "PAD_TOKEN = '<pad>'\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 2\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "RETRAIN = True\n",
        "MODEL_NAME = 'entity_parser'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3f859881",
      "metadata": {
        "id": "3f859881"
      },
      "outputs": [],
      "source": [
        "# fix seed for reproduceability\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beee2157",
      "metadata": {
        "id": "beee2157"
      },
      "source": [
        "## 1- Read data files & Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bca51419",
      "metadata": {
        "id": "bca51419"
      },
      "outputs": [],
      "source": [
        "def read_data(path):\n",
        "    with open(path,'r',encoding='UTF-8') as f:\n",
        "        raw_data = f.readlines()\n",
        "    return raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ca162acf",
      "metadata": {
        "id": "ca162acf"
      },
      "outputs": [],
      "source": [
        "def identify_mention_in_sent(entity: List[str], sentence: List[str]) -> List[str]:\n",
        "    start_index = sentence.index(entity[0])\n",
        "    sentence[start_index:start_index] = [MENTION_OPEN_TOKEN]\n",
        "    end_index = sentence.index(entity[-1])+1\n",
        "    sentence[end_index:end_index] = [MENTION_CLOSE_TOKEN]\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8c752458",
      "metadata": {
        "id": "8c752458"
      },
      "outputs": [],
      "source": [
        "def identify_mention_test(entity: List[str], sentence: List[str]) -> List[str]:\n",
        "    index = [sentence.index(token) for token in sentence if entity[0] in token]\n",
        "    start_index, end_index = index[0], index[0] + len(entity) + 1\n",
        "    sentence[start_index:start_index] = [MENTION_OPEN_TOKEN]\n",
        "    sentence[end_index:end_index] = [MENTION_CLOSE_TOKEN]\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cb22c2f0",
      "metadata": {
        "id": "cb22c2f0"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(dataset: List[str], mode: str):\n",
        "    X = []\n",
        "    Y = []\n",
        "    excluded_training_samples_count = 0\n",
        "    if mode == 'train':\n",
        "        for example in dataset:   \n",
        "            try:\n",
        "                entity, types, sentence =  example.split('\\t')\n",
        "                entity = re.sub(u'[^A-Za-z0-9]+',' ', entity.lower()).split()\n",
        "                types = re.sub(u'[^A-Za-z,\\s]+','', types.lower()).split(', ')\n",
        "                sentence = re.sub(u'[^A-Za-z0-9]+',' ', sentence.lower()).split()\n",
        "\n",
        "                # add special tokens to sentence\n",
        "                sentence = identify_mention_in_sent(entity, sentence)\n",
        "                X.append(sentence)\n",
        "                Y.append(types)\n",
        "            except Exception as e:\n",
        "                excluded_training_samples_count += 1\n",
        "        print('Failed to identify named entity in train data:', excluded_training_samples_count/(len(dataset)))\n",
        "        return X, Y\n",
        "    else:\n",
        "        for example in dataset:  \n",
        "            _, entity, sentence =  example.split('\\t')\n",
        "            entity = re.sub(u'[^A-Za-z0-9]+',' ', entity.lower()).split()\n",
        "            sentence = re.sub(u'[^A-Za-z0-9]+',' ', sentence.lower()).split()\n",
        "\n",
        "            # add special tokens to sentence\n",
        "            sentence = identify_mention_test(entity, sentence)\n",
        "            X.append(sentence)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4c07435d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c07435d",
        "outputId": "532f36da-2f28-4236-99da-4ed70e3cbc24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to identify named entity in train data: 0.002\n"
          ]
        }
      ],
      "source": [
        "git_path = 'https://raw.githubusercontent.com/tsimafeip/LCT-master-course/main/Knowledge_Bases/Lab_03/'\n",
        "\n",
        "# read train file\n",
        "train_file = 'train.tsv'\n",
        "if not os.path.isfile(train_file):\n",
        "    wget.download(git_path+train_file, train_file)\n",
        "\n",
        "train_data = read_data(train_file)\n",
        "train_sents, train_types = preprocess_data(train_data, 'train')\n",
        "\n",
        "# read test file\n",
        "test_file = 'test.tsv'\n",
        "if not os.path.isfile(test_file):\n",
        "    wget.download(git_path+test_file, test_file)\n",
        "\n",
        "test_data = read_data(test_file)\n",
        "test_sents = preprocess_data(test_data, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00528934",
      "metadata": {
        "id": "00528934"
      },
      "source": [
        "## 2- Build source Vocab and target classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "edb6e474",
      "metadata": {
        "id": "edb6e474"
      },
      "outputs": [],
      "source": [
        "class Vocab():\n",
        "    def __init__(self, sentences: List[str]):\n",
        "        self.vocab = self.build_vocab(sentences)\n",
        "        self.unk_index = self.vocab.index(UNK_TOKEN)\n",
        "        self.vocab_idx = self.build_tokens_to_ids_dict()\n",
        "        self.idx_vocab = self.build_ids_to_tokens_dict()\n",
        "        \n",
        "    def build_vocab(self, sents: List[str]) -> List[str]:\n",
        "        all_tokens = [token for sent in sents for token in sent]\n",
        "        vocab = [PAD_TOKEN, UNK_TOKEN, MENTION_OPEN_TOKEN, MENTION_CLOSE_TOKEN]\n",
        "        # add all unique tokens\n",
        "        vocab.extend([token for token in Counter(all_tokens) if token not in vocab])\n",
        "\n",
        "        return vocab\n",
        "    \n",
        "    def build_tokens_to_ids_dict(self):\n",
        "        vocab_idx = {token: i for i, token in enumerate(self.vocab)}\n",
        "        return vocab_idx\n",
        "    \n",
        "    def build_ids_to_tokens_dict(self):\n",
        "        idx_vocab = {i: self.vocab[i] for i in range(len(self.vocab))}\n",
        "        return idx_vocab\n",
        "    \n",
        "    def look_up_indices(self, sentence: List[str]):\n",
        "        return [self.vocab_idx.get(token, self.unk_index) for token in sentence]\n",
        "        \n",
        "    def look_up_tokens(self, indices: List[int]):\n",
        "        return [self.idx_vocab.get(i, UNK_TOKEN) for i in indices]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cc28fa5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc28fa5c",
        "outputId": "b53ffc55-2e8f-4290-9b6d-d7ec641e2431"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51739"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "input_vocab = Vocab(train_sents)\n",
        "len(input_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "81f1dcfd",
      "metadata": {
        "id": "81f1dcfd"
      },
      "outputs": [],
      "source": [
        "# with open('vocab','wb') as ff:\n",
        "#     pickle.dump(input_vocab, ff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5c724e68",
      "metadata": {
        "id": "5c724e68"
      },
      "outputs": [],
      "source": [
        "flat_types = [ty for train_type in train_types for ty in train_type]\n",
        "unique_types = Counter(flat_types)\n",
        "\n",
        "labels = dict(zip(unique_types, range(len(unique_types))))\n",
        "inv_labels = {index: label for label, index in labels.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f643f191",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f643f191",
        "outputId": "51b54b94-602c-410f-dac1-094c94bbe4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 secondary school 3188\n"
          ]
        }
      ],
      "source": [
        "sample_index = 0\n",
        "\n",
        "print(labels[inv_labels[sample_index]], inv_labels[sample_index], len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "00752aaf",
      "metadata": {
        "id": "00752aaf"
      },
      "outputs": [],
      "source": [
        "# with open('inv_labels', 'wb') as ff:\n",
        "#     pickle.dump(inv_labels, ff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c6588ef3",
      "metadata": {
        "id": "c6588ef3"
      },
      "outputs": [],
      "source": [
        "for i in range(len(train_types)):\n",
        "    train_types[i] = [labels[ty] for ty in train_types[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "83945588",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83945588",
        "outputId": "4cd232bd-38a1-420a-8b54-13066f770e9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[304, 474, 475]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "train_types[1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729496ef",
      "metadata": {
        "id": "729496ef"
      },
      "source": [
        "## 3- Create Datasets & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f2d8f1c3",
      "metadata": {
        "id": "f2d8f1c3"
      },
      "outputs": [],
      "source": [
        "class EntityTypingTrainSet(Dataset):\n",
        "    def __init__(self, sents: List[List[str]], types: List[List[int]]):\n",
        "        self.src = sents\n",
        "        self.trgt = types\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        return self.src[index], self.trgt[index]\n",
        "\n",
        "class EntityTypingTestSet(Dataset):\n",
        "    def __init__(self, sents: List[List[str]]):\n",
        "        self.src = sents\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        return self.src[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "526c683b",
      "metadata": {
        "id": "526c683b"
      },
      "outputs": [],
      "source": [
        "# pad examples in the batch to be the same length \n",
        "# NB: COLLATE FUNCTIONS HAVE ACCESS TO INPUT_VOCAB AND LABELS\n",
        "def entity_train_collate_fn(train_batch):\n",
        "    x, y = zip(*train_batch)\n",
        "    src_seqs_len = [len(x_item) for x_item in x]\n",
        "    # get max length for both source and targets \n",
        "    src_max_len = max(src_seqs_len)\n",
        "    \n",
        "    examples = torch.zeros((len(train_batch), src_max_len))\n",
        "    targets = torch.zeros((len(train_batch), len(labels)))\n",
        "\n",
        "    # pad shorter examples to max length for both source and target\n",
        "    for i in range(len(train_batch)):\n",
        "        src = torch.tensor(input_vocab.look_up_indices(train_batch[i][0]))\n",
        "        j = src.size(0)\n",
        "        examples[i] = torch.cat([src, torch.zeros((src_max_len - j))])\n",
        "        targets[i][y[i]] = 1\n",
        "\n",
        "    return examples.long(), targets.long()\n",
        "        \n",
        "\n",
        "def entity_test_collate_fn(test_batch):\n",
        "    src_seqs_len = [len(x_item) for x_item in test_batch]\n",
        "    \n",
        "    # get max length for both source and targets \n",
        "    src_max_len = max(src_seqs_len)\n",
        "    \n",
        "    examples = torch.zeros((len(test_batch), src_max_len))\n",
        "\n",
        "    # pad shorter examples to max length for both source and target\n",
        "    for i in range(len(test_batch)):\n",
        "        src = torch.tensor(input_vocab.look_up_indices(test_batch[i]))\n",
        "        j = src.size(0)\n",
        "        examples[i] = torch.cat([src, torch.zeros((src_max_len - j))])\n",
        "\n",
        "    return examples.long()\n",
        "\n",
        "\n",
        "def build_dataLoader(src_sentences, trgt_labels, mode):\n",
        "    if mode == 'train':\n",
        "        # create dataset and data loader\n",
        "        types_dataset = EntityTypingTrainSet(src_sentences, trgt_labels)\n",
        "        types_dataLoader = DataLoader(types_dataset, batch_size=BATCH_SIZE, collate_fn=entity_train_collate_fn, shuffle=True)\n",
        "        return types_dataLoader\n",
        "    else:\n",
        "        # create dataset and data loader\n",
        "        types_dataset = EntityTypingTestSet(src_sentences)\n",
        "        types_dataLoader = DataLoader(types_dataset, batch_size=TEST_BATCH_SIZE, collate_fn=entity_test_collate_fn)\n",
        "        return types_dataLoader\n",
        "    \n",
        "\n",
        "def all_dataloaders(src_train, trgt_train, src_test):    \n",
        "    train_data_loader = build_dataLoader(src_train, trgt_train, 'train')\n",
        "    test_data_loader = build_dataLoader(src_test, [], 'test')\n",
        "    \n",
        "    return train_data_loader, test_data_loader "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "86213e8c",
      "metadata": {
        "id": "86213e8c"
      },
      "outputs": [],
      "source": [
        "train_data_loader, test_data_loader = all_dataloaders(train_sents, train_types, test_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "680170c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "680170c0",
        "outputId": "b3b35bf9-04d6-4921-f4ef-ebf5b6b31633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 torch.Size([32, 39]) torch.Size([32, 3188])\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "for batch in train_data_loader:\n",
        "    #x, y, _ = batch\n",
        "    print(len(batch), batch[0].size(), batch[1].size())\n",
        "    break\n",
        "\n",
        "for batch in test_data_loader:\n",
        "    #x, y, _ = batch\n",
        "    print(len(batch))\n",
        "    break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a071fa97",
      "metadata": {
        "id": "a071fa97"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2a250877",
      "metadata": {
        "id": "2a250877"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    '''source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html'''\n",
        "    def __init__(self, d_model, dropout, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "bf6b4b17",
      "metadata": {
        "id": "bf6b4b17"
      },
      "outputs": [],
      "source": [
        "class EntityTyper(nn.Module):\n",
        "    '''\n",
        "    Class definition has mostly taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html.\n",
        "    Transformer decoder was replaced with a single linear classification layer.\n",
        "    '''\n",
        "    def __init__(self, vocab_size: int, num_classes: int, model_dim: int,\n",
        "                 num_heads: int, ff_hid_dim: int, num_layers: int,\n",
        "                 dropout_rate: float):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.d_model = model_dim\n",
        "        self.pos_encoder = PositionalEncoding(model_dim, dropout_rate)\n",
        "        self.embedding = nn.Embedding(vocab_size, model_dim)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(model_dim, num_heads, ff_hid_dim, dropout_rate, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.linear = nn.Linear(model_dim, num_classes)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, pad_mask):\n",
        "        # x and x_mask shape: [BATCH_SIZE, PADDED_SEQ_LEN]\n",
        "        assert x.shape == pad_mask.shape\n",
        "\n",
        "        # [BATCH_SIZE, PADDED_SEQ_LEN, MODEL_DIM]\n",
        "        embedded_input = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # [BATCH_SIZE, PADDED_SEQ_LEN, MODEL_DIM]\n",
        "        embedded_input_with_positions = self.pos_encoder(embedded_input)\n",
        "\n",
        "        # [BATCH_SIZE, PADDED_SEQ_LEN, MODEL_DIM]\n",
        "        transformer_encoder_output = self.transformer_encoder(embedded_input_with_positions, None, pad_mask)\n",
        "\n",
        "        # find average vector for all prediction vectors to create a single vector for sentence\n",
        "        # [BATCH_SIZE, MODEL_DIM]\n",
        "        transformer_output = torch.div(torch.sum(transformer_encoder_output, dim=1), transformer_encoder_output.size(1))\n",
        "        \n",
        "        # transform model_dim to num_classes\n",
        "        # [BATCH_SIZE, NUM_CLASSES]\n",
        "        predictions = self.linear(transformer_output)\n",
        "        return predictions\n",
        "    \n",
        "    def train_model(self, dataloader, criterion, optimizer, epoch) -> float:\n",
        "        self.train()\n",
        "        total_loss = 0\n",
        "        print('==================================================')\n",
        "        print('Epoch: {} started'.format(epoch+1))\n",
        "        for batch in tqdm(dataloader):\n",
        "            x, y = batch\n",
        "            \n",
        "            # move batches to device\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # create masks & move masks to device\n",
        "            x_pad_mask = (x == 0).to(device)\n",
        "\n",
        "            # training\n",
        "            self.zero_grad()\n",
        "            out = model(x, x_pad_mask)\n",
        "            loss = criterion(out, y.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print('Epoch: {} - Train Loss: {:.6f}'.format(epoch+1, total_loss))\n",
        "        return total_loss\n",
        "      \n",
        "    def predict_types(self, dataloader):\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for batch in tqdm(dataloader):\n",
        "                x = batch\n",
        "\n",
        "                # move batches to device\n",
        "                x = x.to(device)\n",
        "\n",
        "                # create masks & move masks to device\n",
        "                x_pad_mask = (x == 0).to(device)\n",
        "\n",
        "                # inference\n",
        "                predictions = model(x, x_pad_mask) \n",
        "                for seq, pred_for_seq in zip(x, predictions):\n",
        "                    max_value, max_index = torch.max(pred_for_seq, -1) # get max value in the logit\n",
        "                    predicted_y = max_index.item()\n",
        "                    # test batch size is 1\n",
        "                    yield seq, predicted_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "n9A5hMou7qxM",
      "metadata": {
        "id": "n9A5hMou7qxM"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "VOCAB_SIZE = len(input_vocab)\n",
        "NUM_CLASSES = len(labels)\n",
        "\n",
        "MODEL_DIM = 512\n",
        "FF_HID_DIM = MODEL_DIM * 2\n",
        "NUM_HEADS = 4 \n",
        "NUM_LAYERS = 6\n",
        "DROPOUT_RATE = 0.5\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 25\n",
        "\n",
        "CRITERION = nn.BCEWithLogitsLoss()\n",
        "OPTIMIZER_TYPE = torch.optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "385ca439",
      "metadata": {
        "id": "385ca439"
      },
      "outputs": [],
      "source": [
        "model = EntityTyper(vocab_size=VOCAB_SIZE,\n",
        "                    num_classes=NUM_CLASSES,\n",
        "                    model_dim=MODEL_DIM,\n",
        "                    num_heads=NUM_HEADS,\n",
        "                    ff_hid_dim=FF_HID_DIM,\n",
        "                    num_layers=NUM_LAYERS,\n",
        "                    dropout_rate=DROPOUT_RATE).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d073bd89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d073bd89",
        "outputId": "e1b58e18-9e01-49c7-e28b-84ee0b05570a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<men>', 'shahrak', 'e', 'saqaveh', '</men>', 'persian', 'also', 'romanized', 'as', 'shahrak', 'e', 'saq', 'veh', 'also', 'known', 'as', 'sagawah', 'sagaweh', 'saq', 'veh', 'seh', 'g', 'veh', 'and', 'seq', 'veh', 'is', 'a', 'village', 'in', 'margown', 'rural', 'district', 'margown', 'district', 'boyer', 'ahmad', 'county', 'kohgiluyeh', 'and', 'boyer', 'ahmad', 'province', 'iran', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "[2]\n",
            "torch.Size([32, 3188])\n",
            "torch.Size([32, 3188])\n"
          ]
        }
      ],
      "source": [
        "# example batch and model forward\n",
        "for batch in train_data_loader:\n",
        "    x, y = batch\n",
        "    x_pad_mask = (x == 0).to(device)\n",
        "    \n",
        "    print(input_vocab.look_up_tokens(x[0].tolist()))\n",
        "    print([index for index in range(len(y[0])) if y[0][index] == 1])\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    out = model(x, x_pad_mask)\n",
        "\n",
        "    print(out.size())\n",
        "    print(y.size())\n",
        "    loss = CRITERION(out, y.float())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0f560a",
      "metadata": {
        "id": "ab0f560a"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda445d0",
      "metadata": {
        "id": "cda445d0"
      },
      "outputs": [],
      "source": [
        "if RETRAIN:\n",
        "    optimizer = OPTIMIZER_TYPE(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    start = time.time()\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_loss = model.train_model(dataloader=train_data_loader, \n",
        "                                       criterion=CRITERION, \n",
        "                                       optimizer=optimizer, \n",
        "                                       epoch=epoch)\n",
        "        if epoch_loss < best_loss:\n",
        "            torch.save(model, MODEL_NAME)\n",
        "            best_loss = epoch_loss\n",
        "\n",
        "    end = time.time()\n",
        "    print(f'Training time = {end - start} seconds.')\n",
        "else:\n",
        "    model = torch.load(MODEL_NAME, map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b657da",
      "metadata": {
        "id": "60b657da"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "ad5c0d22",
      "metadata": {
        "id": "ad5c0d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c04c9c-7560-4534-e2d6-9d2ae5360691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 109.03it/s]\n"
          ]
        }
      ],
      "source": [
        "preds = []\n",
        "\n",
        "with open('results.tsv', 'w') as f:\n",
        "    for i, (token_indices, pred_label_index) in enumerate(model.predict_types(test_data_loader)):\n",
        "        test_sample_index = i+1\n",
        "        # we need to output it as list, since multiple type prediction is possible\n",
        "        predicted_types = [inv_labels[pred_label_index]]\n",
        "\n",
        "        preds.append((input_vocab.look_up_tokens(token_indices.tolist()), inv_labels[pred_label_index]))\n",
        "\n",
        "        f.write(f'{test_sample_index}\\t{predicted_types}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "   print(preds[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCHFoPTjNGXq",
        "outputId": "399932f2-cdc7-4f6c-b2be-4f53425f8cc1"
      },
      "id": "SCHFoPTjNGXq",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['<men>', 'jan', '<unk>', '</men>', '1877', '1961', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], 'composer')\n",
            "(['the', '<men>', 'eleventh', 'air', 'force', '</men>', '11', 'af', 'is', 'a', 'numbered', 'air', 'force', 'of', 'the', 'united', 'states', 'air', 'force', 'pacific', 'air', 'forces', '<unk>'], 'military unit')\n",
            "(['<men>', '<unk>', '<unk>', '</men>', '<unk>', 'february', '4', '1916', 'december', '28', '1992', 'was', 'a', 'canadian', 'inuit', 'artist', 'whose', 'preferred', 'medium', 'was', 'a', 'combination', 'of', '<unk>', 'wash', 'and', 'coloured', '<unk>'], 'artist')\n",
            "(['<men>', 'taken', 'the', 'search', 'for', 'sophie', 'parker', '</men>', 'is', 'a', '2013', 'american', 'made', 'for', 'television', 'film', 'directed', 'by', 'don', 'michael', 'paul', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], 'film')\n",
            "(['<men>', 'francisco', '<unk>', '</men>', '29', 'may', '1906', '15', 'september', '1982', 'was', 'a', 'mexican', 'fencer', '<pad>', '<pad>', '<pad>', '<pad>'], 'fencer')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read train file\n",
        "test_groundtruth_file = 'test-groundtruth.tsv'\n",
        "if not os.path.isfile(test_groundtruth_file):\n",
        "    wget.download(git_path+test_groundtruth_file, test_groundtruth_file)\n",
        "\n",
        "evaluation_code = 'evaluate.py'\n",
        "if not os.path.isfile(evaluation_code):\n",
        "    wget.download(git_path+evaluation_code, evaluation_code)\n",
        "\n",
        "! python evaluate.py results.tsv test-groundtruth.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97ZKh-XaO-na",
        "outputId": "67402cfc-c065-4b8e-9cd7-a1ec3a69cb85"
      },
      "id": "97ZKh-XaO-na",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strict: Using exact matching:\n",
            "\tMacro Precision, Recall and F1:\t0.606\t0.5484484126984132\t0.5757896748601857\n",
            "\tMicro Precision, Recall and F1:\t0.606\t0.48635634028892455\t0.5396260017809439\n",
            "Loose: Using exact matching on the lemma of the head-word of the type:\n",
            "\tMacro Precision, Recall and F1:\t0.663\t0.6098755952380952\t0.6353292044494304\n",
            "\tMicro Precision, Recall and F1:\t0.663\t0.5515806988352745\t0.6021798365122616\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ee55c70b"
      ],
      "name": "Task2_Entity_typing.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d234eaca08f81c86c57ba3fab09f675b7f794e918ca74eabd6cec66c7f752238"
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 ('venv_37')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}