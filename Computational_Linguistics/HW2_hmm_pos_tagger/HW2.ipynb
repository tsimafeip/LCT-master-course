{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK9w99ZNMp0a"
      },
      "source": [
        "\n",
        "##Part-of-speech tagging with HMMs##\n",
        "Implement a bigram part-of-speech (POS) tagger based on Hidden Markov Models from scratch. Using NLTK is disallowed, except for the modules explicitly listed below. For this, you will need to develop and/or utilize the following modules:\n",
        "\n",
        "1. Corpus reader and writer (10 points)\n",
        "2. Training procedure (30 points)\n",
        "3. Viterbi tagging, including unknown word handling (50 points) \n",
        "4. Evaluation (10 points)\n",
        "\n",
        "The task is mostly very straightforward, but each step requires careful design. Thus, we suggest you proceed in the following way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQMN4_SuM2ox"
      },
      "source": [
        "\n",
        "##Viterbi algorithm.##\n",
        "\n",
        "First, implement the Viterbi algorithm for finding the optimal state (tag) sequence given the sequence of observations (words). We suggest you test your implementation on a small example for which you know the correct tag sequence, such as the Eisner’s Ice Cream HMM from the lecture.\n",
        "Make sure your Viterbi algorithm runs properly on the example before you proceed to the next step. Submit the best state sequence x that your Viterbi implementation finds for y = 3, 1, 3 and its joint probability P (x, y).\n",
        "There are plenty of other detailed illustrations for the Viterbi algorithm on the Web from which you can take example HMMs. Please resist the temptation to copy Python code from those websites; that would be plagiarism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V8V11XF2PzWs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus.reader.conll import ConllCorpusReader\n",
        "from typing import List, Any, Dict, Tuple\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "id": "EUxKQxoyzk9v"
      },
      "outputs": [],
      "source": [
        "# @title HMM\n",
        "class HMM:\n",
        "    def __init__(\n",
        "            self,\n",
        "            states: List[Any],\n",
        "            state_transition_probs: List[List[float]],\n",
        "            initial_probs: List[float],\n",
        "            emission_probs: List[Dict[Any, float]],\n",
        "    ):\n",
        "        \"\"\"Initializes Hidden Markov Model.\"\"\"\n",
        "\n",
        "        assert len(states) == len(initial_probs)\n",
        "        assert len(states) == len(state_transition_probs)\n",
        "        assert len(states) == len(emission_probs)\n",
        "\n",
        "        self.states = np.asarray(states)\n",
        "        self.state_transition_probs = np.asarray(state_transition_probs)\n",
        "        self.initial_probs = np.asarray(initial_probs)\n",
        "        self.emission_probs = np.asarray(emission_probs)\n",
        "        self._vocab = set()\n",
        "        for emission_dict in self.emission_probs:\n",
        "            for word in emission_dict:\n",
        "                self._vocab.add(word)\n",
        "\n",
        "    def viterbi(self, observations: List[Any]) -> Tuple[List[Any], float]:\n",
        "        \"\"\"Accepts list of observations (words) and returns the optimal state (tag) sequence and its probability.\"\"\"\n",
        "        if not observations:\n",
        "            return [], 0\n",
        "\n",
        "        n, m = len(self.states), len(observations)\n",
        "\n",
        "        backpointers_matrix = np.zeros((n, m), 'int')\n",
        "        viterbi_matrix = np.zeros((n, m))\n",
        "        initial_emissions_vector = \\\n",
        "            [self.emission_probs[state_i][observations[0]] for state_i in range(n)] \\\n",
        "                if observations[0] in self._vocab \\\n",
        "                else [1] * n\n",
        "\n",
        "        viterbi_matrix[:, 0] = self.initial_probs * initial_emissions_vector\n",
        "\n",
        "        for t in range(1, m):\n",
        "            prev_viterbi_col = viterbi_matrix[:, t - 1]\n",
        "            for state_j in range(n):\n",
        "                # we calculate token probability or substituting it with 1 in case on an unknown word\n",
        "                cur_token_prob = self.emission_probs[state_j][observations[t]] if observations[t] in self._vocab else 1\n",
        "                # we multiply previous viterbi column and \n",
        "                # state transitions for all possible states to the chosen 'state_j'\n",
        "                # after this temporary vector is getting multiplied \n",
        "                # by the scalar probability of the current token emission\n",
        "                state_i_to_j_probs = cur_token_prob * prev_viterbi_col * self.state_transition_probs[:, state_j]\n",
        "\n",
        "                # index of the best 'state_i' to move to the 'state_j'\n",
        "                max_index = np.argmax(state_i_to_j_probs)\n",
        "                # remember it in backpointers_matrix\n",
        "                backpointers_matrix[state_j, t] = max_index\n",
        "                # update viterbi matrix\n",
        "                viterbi_matrix[state_j, t] = state_i_to_j_probs[max_index]\n",
        "\n",
        "        last_col_viterbi = viterbi_matrix[:, m - 1]\n",
        "        max_prob_index = np.argmax(last_col_viterbi)\n",
        "        max_prob_value = last_col_viterbi[max_prob_index]\n",
        "\n",
        "        # Deque is used to avoid extra list reversal using appendleft method\n",
        "        path = deque([max_prob_index, ])\n",
        "        for t in range(m-1, 0, -1):\n",
        "            # Invariant: path[0] is t-th element of path\n",
        "            path.appendleft(backpointers_matrix[path[0], t])\n",
        "            # Invariant: path[0] is (t-1)-th element of path\n",
        "\n",
        "        return [self.states[state_i] for state_i in path], max_prob_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDdPGKYJ0ch_",
        "outputId": "3e833395-74d0-4d9f-dc9b-6c65a8cdde8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['H', 'H', 'H'], 0.012544000000000001)\n"
          ]
        }
      ],
      "source": [
        "model_np = HMM(states=[\"H\", \"C\"],\n",
        "            state_transition_probs=[[0.7, 0.3], [0.4, 0.6]],\n",
        "            initial_probs=[0.8, 0.2],\n",
        "            emission_probs=[\n",
        "                {1: 0.2, 2: 0.4, 3: 0.4},\n",
        "                {1: 0.5, 2: 0.4, 3: 0.1},\n",
        "            ])\n",
        "\n",
        "print(model_np.viterbi(observations=[3, 1, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLsmq6s-M-b4"
      },
      "source": [
        "##Training.##\n",
        "\n",
        "Second, learn the parameters of your HMM from data, i.e. the initial, transition, and emission probabilities. Implement a maximum likelihood training procedure for supervised learning of HMMs.\n",
        "You can get a corpus at http://www.coli.uni-saarland.de/~koller/materials/anlp/de-utb.zip. It contains a training set, a test set, and an evaluation set. The training set (de-train.tt) and the evaluation set (de-eval.tt) are written in the commonly used CoNLL format. They are text files with two colums; the first column contains the words, the POS tags are in the second column, and empty lines delimit sentences. The test set (de-test.t) is a copy of the evaluation set with tags stripped, as you should tag the test set using your tagger and then compare your results with the gold-standard ones in the evaluation set. The corpus uses the 12-tag universal POS tagset by Petrov et al. (2012). Feel free to use the NLTK module nltk.corpus.reader (and submodules) for reading the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1PLNBFA3NN_g"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if not os.path.isfile('de-train.tt'):\n",
        "    ! wget http://www.coli.uni-saarland.de/~koller/materials/anlp/de-utb.zip\n",
        "    ! unzip de-utb.zip\n",
        "    ! rm de-utb.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDnd565xNOCH",
        "outputId": "8a0ab3a2-e9c7-413d-a898-101f424c56e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sehr', 'gute', 'Beratung', ',', 'schnelle', ...]\n",
            "[('Sehr', 'ADV'), ('gute', 'ADJ'), ...]\n",
            "[('Sehr', 'ADV'), ('gute', 'ADJ'), ('Beratung', 'NOUN'), (',', '.'), ('schnelle', 'ADJ'), ('Behebung', 'NOUN'), ('der', 'DET'), ('Probleme', 'NOUN'), (',', '.'), ('so', 'ADV'), ('stelle', 'VERB'), ('ich', 'PRON'), ('mir', 'PRON'), ('Kundenservice', 'NOUN'), ('vor', 'PRT'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "train_corpus = ConllCorpusReader(root='.', fileids=['de-train.tt'], columntypes=['words', 'pos'])\n",
        "print(train_corpus.words())\n",
        "print(train_corpus.tagged_words())\n",
        "print(train_corpus.tagged_sents()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6L9y2cYAAZ"
      },
      "source": [
        "We need to know the set of hidden states. Here is an excerpt from Petrov's article:\n",
        "\n",
        "\"Our universal POS tagset unifies this previous work and defines the following twelve POS tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), ‘.’ (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XuFzzNayYH9h"
      },
      "outputs": [],
      "source": [
        "states = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'NUM', 'CONJ', 'PRT', '.', 'X']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k86SVCl9i6Jd"
      },
      "source": [
        "Here is the theory from Jurafski and Martin book on supervised training for HMM:\n",
        "\n",
        "X: sequence of hidden states / tags $q_1, ... , q_t$ <br>\n",
        "Y: the corresponding sequence of observations / words $o_1, ... ,o_t$\n",
        "\n",
        "initial probabilities: $c_j = \\frac{\\text{#sentences with } X_1=q_j}{\\text{#sentences}}$ <br>\n",
        "\n",
        "\n",
        "transition probabilities: $a_{ij}$ = $\\frac{C(X_t = q_i, X_{t+1} = q_j)}{C(X_t = q_i)}$ <br>\n",
        "\n",
        "emission probabilities: $b_j(o) = \\frac{C(X_t = q_j, Y_t = o)}{C(X_t = q_j)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "code",
        "id": "YxhcYSAlgZFN"
      },
      "outputs": [],
      "source": [
        "#@title HMM training\n",
        "def train_HMM_params(states: List[Any], corpus: ConllCorpusReader) -> Tuple[\n",
        "    List[List[float]], List[float], List[Dict[Any, float]]]:\n",
        "    n = len(states)\n",
        "    state_to_index = {state: i for i, state in enumerate(states)}\n",
        "\n",
        "    state_occurence = np.zeros(n)\n",
        "    state_occurence_with_transition = np.zeros(n)\n",
        "    state_cooccurence_matrix = np.zeros((n, n))\n",
        "    state_to_word_counter = [defaultdict(int) for _ in range(n)]\n",
        "    states_for_sentence_start = np.zeros(n)\n",
        "\n",
        "    for tagged_sentence in tqdm(corpus.tagged_sents()):\n",
        "        for i, (observed_word, word_state) in enumerate(tagged_sentence):\n",
        "            state_index = state_to_index[word_state]\n",
        "            if i == 0:\n",
        "                states_for_sentence_start[state_index] += 1\n",
        "            if i != len(tagged_sentence) - 1:\n",
        "                # we have a transition to the next tag (current state is the first item in bigram)\n",
        "                state_occurence_with_transition[state_index] += 1\n",
        "                next_state = tagged_sentence[i + 1][1]\n",
        "                next_state_index = state_to_index[next_state]\n",
        "                state_cooccurence_matrix[state_index][next_state_index] += 1\n",
        "            state_to_word_counter[state_index][observed_word] += 1\n",
        "            state_occurence[state_index] += 1\n",
        "\n",
        "    # it is just a smart way to divide each row by the correspoding vector element\n",
        "    state_transition_probs = state_cooccurence_matrix / state_occurence_with_transition[:, None]\n",
        "\n",
        "    initial_probs = states_for_sentence_start / len(corpus.tagged_sents())\n",
        "\n",
        "    emission_probs = [defaultdict(int) for _ in range(n)]\n",
        "    for state_i, counted_words in enumerate(state_to_word_counter):\n",
        "        for word, word_count in counted_words.items():\n",
        "            emission_probs[state_i][word] = word_count / state_occurence[state_i]\n",
        "\n",
        "    return state_transition_probs, initial_probs, emission_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B2WDdkzhH56",
        "outputId": "919a56ae-44ae-4085-95a6-0e71d79f05dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14118/14118 [00:02<00:00, 6869.75it/s]\n"
          ]
        }
      ],
      "source": [
        "trained_state_transition_probs, trained_initial_probs, trained_emission_probs = train_HMM_params(states, train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyiINXb7NOUh"
      },
      "source": [
        "##Evaluation.##\n",
        "\n",
        "Once you have trained a model, evaluate it on the unseen data from the test set. Run the Viterbi algorithm with each of your models, and output a tagged corpus in the two-column CoNLL format (*.tt). We will provide an evaluation script on Classroom. Run it on the output of your tagger and the evaluation set and report your results.\n",
        "Note that your tagger will initially fail to produce output for sentences that contain words you haven’t seen in training. If you have such a word w appear at sentence position t, you will have bj(w) = 0 for all states/tags j, and therefore Vt(j) = 0 for all j. Adapt your tagger by implementing the following crude approach to unknown words. Whenever you get Vt(j) = 0 for all j because of an unknown word w at position t, pretend that bj(w) = 1 for all j. This will basically set Vt(j) = maxi Vt−1(i) · aij, and allow you to interpolate the missing POS tag based on the transition probabilities alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHEN8JCzlLks",
        "outputId": "1e5bca6a-b4a2-40d5-c8e6-59fd05692f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Der', 'Hauptgang', 'war', 'in', 'Ordnung', ',', 'aber', 'alles', 'andere', 'als', 'umwerfend', '.']\n"
          ]
        }
      ],
      "source": [
        "test_corpus = ConllCorpusReader(root='.', fileids=['de-test.t'], columntypes=['words'])\n",
        "print(test_corpus.sents()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jG49ExXKlB-_"
      },
      "outputs": [],
      "source": [
        "model = HMM(states=states,\n",
        "            state_transition_probs=trained_state_transition_probs,\n",
        "            initial_probs=trained_initial_probs,\n",
        "            emission_probs=trained_emission_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sg3idcOosacH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! wget https://raw.githubusercontent.com/tsimafeip/LCT-master-course/main/Computational_Linguistics/HW2_hmm_pos_tagger/eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3hep5sfovjGc"
      },
      "outputs": [],
      "source": [
        "# with open('de-train-res.tt', 'w') as f:\n",
        "#     for sentence in tqdm(train_corpus.sents()):\n",
        "#         predicted_tags, max_prob_value = model.viterbi(observations=sentence)\n",
        "    \n",
        "#         for tag, word in zip(predicted_tags, sentence):\n",
        "#             f.write(f\"{word}\\t{tag}\\n\")\n",
        "#         f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h03JWaTvvuM9"
      },
      "outputs": [],
      "source": [
        "# ! python eval.py de-train.tt de-train-res.tt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IBhd7fFmLj7",
        "outputId": "39bb9e7b-1717-45a8-e54b-001d924f2256"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 522.54it/s]\n"
          ]
        }
      ],
      "source": [
        "with open('de-test-res.tt', 'w') as f:\n",
        "    for sentence in tqdm(test_corpus.sents()):\n",
        "        predicted_tags, max_prob_value = model.viterbi(observations=sentence)\n",
        "    \n",
        "        for tag, word in zip(predicted_tags, sentence):\n",
        "            f.write(f\"{word}\\t{tag}\\n\")\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9eArQp8r83W",
        "outputId": "d4cf7ead-c6c0-430d-d141-78747d8bd7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparing gold file \"de-eval.tt\" and system file \"de-test-res.tt\"\n",
            "\n",
            "Precision, recall, and F1 score:\n",
            "\n",
            "  DET 0.8232 0.9755 0.8929\n",
            " NOUN 0.9296 0.9141 0.9218\n",
            " VERB 0.9202 0.9211 0.9206\n",
            "  ADP 0.9348 0.9775 0.9557\n",
            "    . 0.9608 1.0000 0.9800\n",
            " CONJ 0.9498 0.8974 0.9228\n",
            " PRON 0.8671 0.8364 0.8515\n",
            "  ADV 0.9043 0.8058 0.8523\n",
            "  ADJ 0.8099 0.7222 0.7635\n",
            "  NUM 0.9905 0.7704 0.8667\n",
            "  PRT 0.8712 0.9251 0.8973\n",
            "    X 0.2222 0.0909 0.1290\n",
            "\n",
            "Accuracy: 0.9095\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! python eval.py de-eval.tt de-test-res.tt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcTZCCzwNYXQ"
      },
      "source": [
        "##Extra credit.##\n",
        "\n",
        "The task is challenging as it stands. However, feel free to go further for extra credit, e.g. by doing one of the following: implement better unknown word handling, use a trigram tagger, plot a learning curve for your tagger (accuracy as a function of training data size), plot a speed vs. sentence length curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fY9hgA2ZNc2M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm1-RTTANdP3"
      },
      "source": [
        "Please submit your code, instructions for running your tagger and tagging output(s). Document any additional data you submit. With this, you will have implemented your first POS tagger! Well done!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
