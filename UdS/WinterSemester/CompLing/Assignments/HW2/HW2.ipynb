{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK9w99ZNMp0a"
      },
      "source": [
        "\n",
        "##Part-of-speech tagging with HMMs##\n",
        "Implement a bigram part-of-speech (POS) tagger based on Hidden Markov Models from scratch. Using NLTK is disallowed, except for the modules explicitly listed below. For this, you will need to develop and/or utilize the following modules:\n",
        "\n",
        "1. Corpus reader and writer (10 points)\n",
        "2. Training procedure (30 points)\n",
        "3. Viterbi tagging, including unknown word handling (50 points) \n",
        "4. Evaluation (10 points)\n",
        "\n",
        "The task is mostly very straightforward, but each step requires careful design. Thus, we suggest you proceed in the following way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQMN4_SuM2ox"
      },
      "source": [
        "\n",
        "##Viterbi algorithm.##\n",
        "\n",
        "First, implement the Viterbi algorithm for finding the optimal state (tag) sequence given the sequence of observations (words). We suggest you test your implementation on a small example for which you know the correct tag sequence, such as the Eisner’s Ice Cream HMM from the lecture.\n",
        "Make sure your Viterbi algorithm runs properly on the example before you proceed to the next step. Submit the best state sequence x that your Viterbi implementation finds for y = 3, 1, 3 and its joint probability P (x, y).\n",
        "There are plenty of other detailed illustrations for the Viterbi algorithm on the Web from which you can take example HMMs. Please resist the temptation to copy Python code from those websites; that would be plagiarism."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8V11XF2PzWs"
      },
      "source": [
        "from typing import List, Any, Dict, Tuple\n",
        "import numpy as np\n",
        "from collections import deque"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5o3k_bgSV2U",
        "cellView": "form"
      },
      "source": [
        "#@title Basic HMM\n",
        "class HMM:\n",
        "    def __init__(\n",
        "            self,\n",
        "            states: List[Any],\n",
        "            state_transition_probs: List[List[float]],\n",
        "            initial_probs: List[float],\n",
        "            emission_probs: List[Dict[Any, float]],\n",
        "    ):\n",
        "        \"\"\"Initializes Hidden Markov Model.\"\"\"\n",
        "\n",
        "        assert len(states) == len(initial_probs)\n",
        "        assert len(states) == len(state_transition_probs)\n",
        "        assert len(states) == len(emission_probs)\n",
        "\n",
        "        self.states = states\n",
        "        self.state_transition_probs = state_transition_probs\n",
        "        self.initial_probs = initial_probs\n",
        "        self.emission_probs = emission_probs\n",
        "\n",
        "    def viterbi(self, observations: List[Any]) -> Tuple[List[Any], float]:\n",
        "        \"\"\"Accepts list of observations (words) and returns the optimal state (tag) sequence and its probability.\"\"\"\n",
        "        if not observations:\n",
        "            return [], 0\n",
        "\n",
        "        viterbi_matrix = [[0 for j in range(len(observations))] for i in range(len(self.states))]\n",
        "        backpointers_matrix = [[0 for j in range(len(observations))] for i in range(len(self.states))]\n",
        "\n",
        "        for state_i in range(len(self.states)):\n",
        "            viterbi_matrix[state_i][0] = self.initial_probs[state_i] * self.emission_probs[state_i][observations[0]]\n",
        "\n",
        "        for t in range(1, len(observations)):\n",
        "            for state_j in range(len(self.states)):\n",
        "                cur_token_prob = self.emission_probs[state_j][observations[t]]\n",
        "                state_i_to_j_probs = [\n",
        "                    viterbi_matrix[state_i][t - 1] * cur_token_prob * self.state_transition_probs[state_i][state_j]\n",
        "                    for state_i in range(len(self.states))\n",
        "                ]\n",
        "\n",
        "                max_index = np.argmax(state_i_to_j_probs)\n",
        "                backpointers_matrix[state_j][t] = max_index\n",
        "                viterbi_matrix[state_j][t] = state_i_to_j_probs[max_index]\n",
        "\n",
        "        last_col_viterbi = [viterbi_matrix[i][len(observations) - 1] for i in range(len(self.states))]\n",
        "        max_prob_index = np.argmax(last_col_viterbi)\n",
        "        max_prob_value = last_col_viterbi[max_prob_index]\n",
        "\n",
        "        # Deque is used to avoid extra list reversal using appendleft method\n",
        "        path = deque([max_prob_index, ])\n",
        "        for t in range(len(observations) - 2, -1, -1):\n",
        "            path.appendleft(backpointers_matrix[path[0]][t])\n",
        "\n",
        "        return [self.states[state_i] for state_i in path], np.round(max_prob_value, 4)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrhxiNKoM-Av",
        "outputId": "4c83326e-c9d8-4f54-d380-98c1dbf141f6"
      },
      "source": [
        "model = HMM(states=[\"H\", \"C\"],\n",
        "            state_transition_probs=[[0.7, 0.3], [0.4, 0.6]],\n",
        "            initial_probs=[0.8, 0.2],\n",
        "            emission_probs=[\n",
        "                {1: 0.2, 2: 0.4, 3: 0.4},\n",
        "                {1: 0.5, 2: 0.4, 3: 0.1},\n",
        "            ])\n",
        "\n",
        "print(model.viterbi(observations=[3, 1, 3]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['H', 'H', 'H'], 0.0125)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EUxKQxoyzk9v"
      },
      "source": [
        "#@title Refactored HMM with numpy\n",
        "\n",
        "# Here is a small experiment with numpy instead of standard pythos lists.\n",
        "# Eventually, code looks more consice thanks to native vector operations.\n",
        "class HMM_np:\n",
        "    def __init__(\n",
        "            self,\n",
        "            states: List[Any],\n",
        "            state_transition_probs: List[List[float]],\n",
        "            initial_probs: List[float],\n",
        "            emission_probs: List[Dict[Any, float]],\n",
        "    ):\n",
        "        \"\"\"Initializes Hidden Markov Model.\"\"\"\n",
        "\n",
        "        assert len(states) == len(initial_probs)\n",
        "        assert len(states) == len(state_transition_probs)\n",
        "        assert len(states) == len(emission_probs)\n",
        "\n",
        "        self.states = np.asarray(states)\n",
        "        self.state_transition_probs = np.asarray(state_transition_probs)\n",
        "        self.initial_probs = np.asarray(initial_probs)\n",
        "        self.emission_probs = np.asarray(emission_probs)\n",
        "\n",
        "    def viterbi(self, observations: List[Any]) -> Tuple[List[Any], float]:\n",
        "        \"\"\"Accepts list of observations (words) and returns the optimal state (tag) sequence and its probability.\"\"\"\n",
        "        if not observations:\n",
        "            return [], 0\n",
        "\n",
        "        n, m = len(self.states), len(observations)\n",
        "\n",
        "        backpointers_matrix = np.zeros((n, m), 'int')\n",
        "        viterbi_matrix = np.zeros((n, m))\n",
        "        initial_emissions_vector = [self.emission_probs[state_i][observations[0]] for state_i in range(n)]\n",
        "        viterbi_matrix[:, 0] = self.initial_probs * initial_emissions_vector\n",
        "\n",
        "        for t in range(1, m):\n",
        "            for state_j in range(n):\n",
        "                cur_token_prob = self.emission_probs[state_j][observations[t]]\n",
        "                state_i_to_j_probs = cur_token_prob * viterbi_matrix[:, t - 1] * self.state_transition_probs[:, state_j]\n",
        "\n",
        "                max_index = np.argmax(state_i_to_j_probs)\n",
        "                backpointers_matrix[state_j, t] = max_index\n",
        "                viterbi_matrix[state_j, t] = state_i_to_j_probs[max_index]\n",
        "\n",
        "        last_col_viterbi = viterbi_matrix[:, m - 1]\n",
        "        max_prob_index = np.argmax(last_col_viterbi)\n",
        "        max_prob_value = last_col_viterbi[max_prob_index]\n",
        "\n",
        "        # Deque is used to avoid extra list reversal using appendleft method\n",
        "        path = deque([max_prob_index, ])\n",
        "        for t in range(len(observations) - 2, -1, -1):\n",
        "            path.appendleft(backpointers_matrix[path[0]][t])\n",
        "\n",
        "        return [self.states[state_i] for state_i in path], np.round(max_prob_value, 4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDdPGKYJ0ch_",
        "outputId": "f4668c9f-962d-4731-f4dc-280674659c46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_np = HMM_np(states=[\"H\", \"C\"],\n",
        "            state_transition_probs=[[0.7, 0.3], [0.4, 0.6]],\n",
        "            initial_probs=[0.8, 0.2],\n",
        "            emission_probs=[\n",
        "                {1: 0.2, 2: 0.4, 3: 0.4},\n",
        "                {1: 0.5, 2: 0.4, 3: 0.1},\n",
        "            ])\n",
        "\n",
        "print(model_np.viterbi(observations=[3, 1, 3]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['H', 'H', 'H'], 0.0125)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLsmq6s-M-b4"
      },
      "source": [
        "##Training.##\n",
        "\n",
        "Second, learn the parameters of your HMM from data, i.e. the initial, transition, and emission probabilities. Implement a maximum likelihood training procedure for supervised learning of HMMs.\n",
        "You can get a corpus at http://www.coli.uni-saarland.de/~koller/materials/anlp/de-utb.zip. It contains a training set, a test set, and an evaluation set. The training set (de-train.tt) and the evaluation set (de-eval.tt) are written in the commonly used CoNLL format. They are text files with two colums; the first column contains the words, the POS tags are in the second column, and empty lines delimit sentences. The test set (de-test.t) is a copy of the evaluation set with tags stripped, as you should tag the test set using your tagger and then compare your results with the gold-standard ones in the evaluation set. The corpus uses the 12-tag universal POS tagset by Petrov et al. (2012). Feel free to use the NLTK module nltk.corpus.reader (and submodules) for reading the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PLNBFA3NN_g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDnd565xNOCH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyiINXb7NOUh"
      },
      "source": [
        "##Evaluation.##\n",
        "\n",
        "Once you have trained a model, evaluate it on the unseen data from the test set. Run the Viterbi algorithm with each of your models, and output a tagged corpus in the two-column CoNLL format (*.tt). We will provide an evaluation script on Classroom. Run it on the output of your tagger and the evaluation set and report your results.\n",
        "Note that your tagger will initially fail to produce output for sentences that contain words you haven’t seen in training. If you have such a word w appear at sentence position t, you will have bj(w) = 0 for all states/tags j, and therefore Vt(j) = 0 for all j. Adapt your tagger by implementing the following crude approach to unknown words. Whenever you get Vt(j) = 0 for all j because of an unknown word w at position t, pretend that bj(w) = 1 for all j. This will basically set Vt(j) = maxi Vt−1(i) · aij, and allow you to interpolate the missing POS tag based on the transition probabilities alone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uq--zqyNYGO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0iv_I66NYIo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcTZCCzwNYXQ"
      },
      "source": [
        "##Extra credit.##\n",
        "\n",
        "The task is challenging as it stands. However, feel free to go further for extra credit, e.g. by doing one of the following: implement better unknown word handling, use a trigram tagger, plot a learning curve for your tagger (accuracy as a function of training data size), plot a speed vs. sentence length curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY9hgA2ZNc2M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm1-RTTANdP3"
      },
      "source": [
        "Please submit your code, instructions for running your tagger and tagging output(s). Document any additional data you submit. With this, you will have implemented your first POS tagger! Well done!"
      ]
    }
  ]
}